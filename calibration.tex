\subsection{Runtime Calibration} \label{sec:calibration}
During RAT, we differentiate between known and unknown rts's. 
Known rts's are those which have been encountered during DTA. So the optimal configuration for the rts's is known. 
However, unknown rts's describe those which have not been encountered during DTA. 
There are several reasons why unseen rts's might occur. 
First, an unseen rts may consists of already known regions, but with unknown user parameters that were not present during DTA or parameters that might have changed between DTA and runtime. Typically, these changes are related to different application inputs. 
Second, an unseen rts may consist of completely new regions, which were not seen during DTA. 
The goal of the calibration is to handle these unknown rts's during RAT.

Since, the calibration is done during the production run, there are a few restrictions which had to be taken in account for the design of calibration mechanism. 
First, there can be no user input and 
second, a good configuration has to be found in short time. 
A DTA like approach for searching optimal configuration is not feasible as it would degrade the performance of the application.
To avoid this a Machine Learning based method is used to
determine a good configuration for an unseen rts. 
Using this method we could split the calibration in a training part and a detecting part. 
The training is done once per HPC system and described below. During RAT the trained model is used to detect a good configuration.
Once a configuration is found, it is stored in the TMM.

\subsubsection{Data Basis:}
Each Machine Learning algorithm needs a data basis, also called \textit{feature vector}, to learn from. 
For supervised learning, an \textit{optimization criterion} and a \textit{target vector} are needed as well. 
In our case, the \textit{optimization criterion} is to reduce the energy consumption of certain program functions. 
To do so, we change the frequency of the processor core and uncore, which represents the \textit{target vector}. 
The training examples are generated by different energy optimal frequencies for monitored program functions. 
As feature vector, we use the hardware performance measurement counters (PMCs) as the data basis to learn from and predict a
good configuration.

PMCs are CPU registers that can be used to count different hardware events, like executed instructions, cache accesses, and branch predictions.
Modern processor architectures are equipped with counters that measure events related to the processing units and core-related caches, and counters that observe the behavior of shared components within the uncore \cite{molka:2017:a}.
A detailed description for core and uncore counters on the Intel
Haswell-EP platform can be found in \cite{Intel2018} and \cite{xeon_e5v3_Uncore}, respectively.

The purpose of the learning algorithm is to use these counters to predict the most energy efficient configuration of the processor frequency for a specific region. However, as there is only a limited number of PMCs available to measure the different events, it is not possible to collect all events at the same time. Hence, we need to find the appropriate core and uncore events.

\subsubsection{Selecting Relevant Performance Events:}

Suitable hardware events are those that contribute to a decision about the optimal frequencies.
However, to gain the most accurate information, we need to filter out redundant information. 
We use the \textit{Correlation-based feature selection} approach proposed by Yoo \cite{Yoo2012} to choose events without reduntant information. 
To generate the input vectors for the learning algorithm, we remeasure
the chosen relevant events to generate new feature vectors.

\subsubsection{The Learning algorithm:}
For the implementation of the learning algorithm we chose Neural Networks (NN). 
Neural networks try to mimic the human brain, where computational units often referred to as \textit{neurons} combine input values to produce an output value \cite{Haykin2009}.
The aim is to collect feature vectors for a given configuration of the processor frequency that results in the lowest energy consumption. Moreover, in the training step, this data is normalized by the runtime of the region and used as input to the NN. The optimal processor
frequency for each region is termed as output.

\subsubsection{Training and Runtime:}
For tuning of the ML algorithm, the PTF is replaced by the tuning framework in Figure~\ref{fig:rrl} and PMCs are given as input. 
For training, SPEC OpenMP benchmark has been used. The benchmarks are run on our target platform \ref{sec:results} and different performance counters are collected. 
At the end of each benchmark run, the collected counter
data is saved and evaluated by the training framework. 
If the collected counter data are sufficient, the framework will proceed. Otherwise, the benchmark is executed again. 
Once the training satisfies the quality requirements set, the framework determines which events are the most interesting ones.

Now, at runtime during a production run, when the RTS-Handler
detects the presence of an unseen rts, instead of using a system configuration found during DTA, the calibration module specifies a configuration and starts collecting values from the pre-selected set of counters.
At a pre-determined point in the program, for example, after 100 iterations, the collected counter values are fed to the learning algorithm, which then determines the best configuration to apply for future occurrences of the same rts.